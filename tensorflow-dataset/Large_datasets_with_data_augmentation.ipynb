{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full TensorFlow input and training pipeline for large datasets with data augmentation\n",
    "\n",
    "## Introduction\n",
    "\n",
    "The new high level Dataset API makes it quite easy to deal with large datasets. No need to bother with queues anymore.\n",
    "\n",
    "This notebook gives a short example of how to use the Dataset API for:\n",
    "\n",
    "1. loading multiple files for each input example,\n",
    "2. data augmentation.\n",
    "3. training and computing validation loss\n",
    "\n",
    "Loading multiple files for each input example can be needed for several applications:\n",
    "\n",
    "* dealing with image sequences,\n",
    "* object detection with multiple cameras,\n",
    "* etc.\n",
    "\n",
    "Data augmentation is a classical problem in deep learning. However, the Dataset API documentation does not give any hint about how to achieve data augmentation. I am not sure that the method used below is the best for this task. If you find a better way, don't hesitate to tell me!\n",
    "\n",
    "__Note on tensorflow version:__\n",
    "\n",
    "> In tensorflow v1.4, *tf.contrib.data* has been integrated into *tf.data*. Please use appropriate import according to your tensorflow version. See comment in code below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's go!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os.path\n",
    "\n",
    "# For tensorflow 1.3\n",
    "#from tensorflow.contrib import data as tfdata\n",
    "\n",
    "# For tensorflow 1.4 and above\n",
    "from tensorflow import data as tfdata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create some dummy data\n",
    "\n",
    "We are going to create some dummy data files. In a real application, use your own data files. Here, we will create 4 files: two groups of two files. Each file will contain a 3x3 matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "height = 3\n",
    "width = 3\n",
    "\n",
    "# Create random files containing raw matrix of shape 3x3\n",
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        # Create matrix with fake data\n",
    "        matrix = np.zeros((height,width)) + i + (j*10)\n",
    "        # Save matrix as raw float32 file\n",
    "        matrix.astype('float32').tofile('data/file_' + str(i) + '_' + str(j) + '.raw')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at one of our dummy data files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 11.  11.  11.]\n",
      " [ 11.  11.  11.]\n",
      " [ 11.  11.  11.]]\n"
     ]
    }
   ],
   "source": [
    "# Print one of the generated files for checking\n",
    "matrix = np.fromfile('data/file_1_1.raw', dtype=np.float32)\n",
    "matrix = matrix.reshape((height,width))\n",
    "print(matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, our dummy data file looks good."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create parser\n",
    "\n",
    "We will need a parser to read our examples from the data files.\n",
    "\n",
    "Here, each example will be created by stacking data coming from two different files and a label. Therefore, the parser arguments will be the two filenames and the label. It will return two tensors containing the example and the label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create parser\n",
    "# Args: filenames\n",
    "# Returns: tensor containing read and decoded element\n",
    "def _parse_data(filename0, filename1, label):\n",
    "    # Read and decode first file\n",
    "    matrix0 = tf.read_file(filename0)\n",
    "    matrix0 = tf.decode_raw(matrix0, out_type=tf.float32)\n",
    "    matrix0 = tf.reshape(matrix0, [height,width])\n",
    "    \n",
    "    # Read and decode second file\n",
    "    matrix1 = tf.read_file(filename1)\n",
    "    matrix1 = tf.decode_raw(matrix1, out_type=tf.float32)\n",
    "    matrix1 = tf.reshape(matrix1, [height,width])\n",
    "    \n",
    "    # Stack the two elements together\n",
    "    X = tf.stack([matrix0, matrix1])\n",
    "    \n",
    "    # Get label (you could implement more complex logic here if needed)\n",
    "    y = tf.reshape(label, [1])\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create data augmentation function\n",
    "\n",
    "We will need a function to implement the data augmentation logic.\n",
    "\n",
    "This function takes one example (X,y), and returns a dataset with two examples: the original example, and a second one generated on the fly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Data augmentation function: create several examples from one example\n",
    "# Args: One example X,y\n",
    "# Returns: Dataset containing several examples, after data augmentation\n",
    "def _data_augment(X,y):\n",
    "    # Generate new data from example X\n",
    "    X0 = X\n",
    "    X1 = -X # Dummy data augmentation, but we could use any transformation we need. We could also generate more examples.\n",
    "    X_augmented = tf.stack([X0,X1])\n",
    "    \n",
    "    # Repeat y\n",
    "    y_augmented = tf.stack([y,y])\n",
    "    \n",
    "    dataset = tfdata.Dataset.from_tensor_slices((X_augmented, y_augmented))\n",
    "        \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### It's almost done\n",
    "\n",
    "Now that we have implemented our parsing and data augmentation logic, we can create the dataset.\n",
    "\n",
    "As you can see, with the TensorFlow Dataset API, it is really easy! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "####################################################################\n",
    "# Create dataset from files\n",
    "####################################################################\n",
    "\n",
    "# Get the filenames lists\n",
    "filenames0 = ['data/file_0_0.raw', 'data/file_0_1.raw']\n",
    "filenames1 = ['data/file_1_0.raw', 'data/file_1_1.raw']\n",
    "\n",
    "# Create tensorflow constant containing the filenames\n",
    "tf_filenames0 = tf.constant(filenames0)\n",
    "tf_filenames1 = tf.constant(filenames1)\n",
    "\n",
    "# Create labels dataset\n",
    "labels = tf.constant([15, 25])\n",
    "\n",
    "# Create dataset containing filenames and labels\n",
    "dataset = tfdata.Dataset.from_tensor_slices((tf_filenames0, tf_filenames1, labels))\n",
    "\n",
    "# Use our _parse_data function to read files and decode data\n",
    "dataset = dataset.map(_parse_data)\n",
    "\n",
    "#####################################################################\n",
    "# Data augmentation\n",
    "#####################################################################\n",
    "\n",
    "# Apply data augmentation\n",
    "dataset = dataset.interleave(_data_augment, cycle_length=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print the dataset\n",
    "\n",
    "We can now print the dataset to check that everything is working."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([[[ 0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.]],\n",
      "\n",
      "       [[ 1.,  1.,  1.],\n",
      "        [ 1.,  1.,  1.],\n",
      "        [ 1.,  1.,  1.]]], dtype=float32), array([15]))\n",
      "(array([[[-0., -0., -0.],\n",
      "        [-0., -0., -0.],\n",
      "        [-0., -0., -0.]],\n",
      "\n",
      "       [[-1., -1., -1.],\n",
      "        [-1., -1., -1.],\n",
      "        [-1., -1., -1.]]], dtype=float32), array([15]))\n",
      "(array([[[ 10.,  10.,  10.],\n",
      "        [ 10.,  10.,  10.],\n",
      "        [ 10.,  10.,  10.]],\n",
      "\n",
      "       [[ 11.,  11.,  11.],\n",
      "        [ 11.,  11.,  11.],\n",
      "        [ 11.,  11.,  11.]]], dtype=float32), array([25]))\n",
      "(array([[[-10., -10., -10.],\n",
      "        [-10., -10., -10.],\n",
      "        [-10., -10., -10.]],\n",
      "\n",
      "       [[-11., -11., -11.],\n",
      "        [-11., -11., -11.],\n",
      "        [-11., -11., -11.]]], dtype=float32), array([25]))\n",
      "End of dataset\n"
     ]
    }
   ],
   "source": [
    "#####################################################################\n",
    "# Print the dataset\n",
    "#####################################################################\n",
    "\n",
    "# create TensorFlow Iterator object\n",
    "iterator = dataset.make_one_shot_iterator()\n",
    "next_element = iterator.get_next()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "\n",
    "    # iterate over the dataset\n",
    "    while True:\n",
    "        try:\n",
    "            elem = sess.run(next_element)\n",
    "            print(elem)\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            print(\"End of dataset\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### It is working!\n",
    "\n",
    "We can see that parsing and data augmentation work well:\n",
    "\n",
    "* each example contains:\n",
    "  * two matrices coming from two different files\n",
    "  * a label\n",
    "* after each example, there is an augmented example (-X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's train our model now\n",
    "\n",
    "### Create a model\n",
    "First we create a small dummy model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a small dummy model\n",
    "def model_function(input_data):\n",
    "    flattened = tf.contrib.layers.flatten(input_data)\n",
    "    a1 = tf.layers.dense(flattened, 100, activation=tf.nn.relu)\n",
    "    output = tf.layers.dense(a1, 1)\n",
    "    return output\n",
    "\n",
    "# Create the loss function    \n",
    "def loss_function(prediction, label):\n",
    "    return tf.losses.mean_squared_error(prediction, label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will do the following:\n",
    "\n",
    "* Prepare the dataset for training\n",
    "  * Repeat the dataset (remove this in real application, we do it only because in this example we have too few data)\n",
    "  * Set the batch size\n",
    "  * Create an feedable iterator on the batched dataset. The feedable iterator will allow us to switch between training and validation dataset, see documentation here: https://www.tensorflow.org/programmers_guide/datasets\n",
    "* Create a global_step variable. This variable will count the number of training steps. It will be saved with the model, and will allow us to resume training.\n",
    "* Create the predict op, loss op and train op."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repeat dataset in order to simulate more data (remove this in real application)\n",
    "batch_dataset = dataset.repeat(30)\n",
    "\n",
    "# Set batch size\n",
    "batch_dataset = batch_dataset.batch(2)\n",
    "\n",
    "# create TensorFlow Iterator object\n",
    "handle = tf.placeholder(tf.string, shape=[])\n",
    "iterator = tf.data.Iterator.from_string_handle(\n",
    "    handle, batch_dataset.output_types, batch_dataset.output_shapes)\n",
    "\n",
    "# Create training_iterator\n",
    "training_iterator = batch_dataset.make_initializable_iterator()\n",
    "\n",
    "# Get example, compute loss and create optimizer\n",
    "next_batch_examples, next_batch_labels = iterator.get_next()\n",
    "\n",
    "# Create global_step variable, to store the global training step\n",
    "global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "\n",
    "# Create ops that we will use during training, loss computation and prediction\n",
    "predict_op = model_function(next_batch_examples)\n",
    "loss_op = loss_function(predict_op, next_batch_labels)\n",
    "train_op = tf.train.AdamOptimizer(1e-3).minimize(loss_op, global_step=global_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### That's all. We can train!\n",
    "\n",
    "We start a training session. The training process is quite standard, see https://www.tensorflow.org/get_started/mnist/mechanics for more details.\n",
    "\n",
    "In this example, we will also print training loss periodically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch:  0\n",
      "Training loss at iteration 0: 218.800171 \n",
      "Training loss at iteration 25: 215.280466 \n",
      "Training loss at iteration 50: 159.478886 \n",
      "--------------\n",
      "Starting epoch:  1\n",
      "Training loss at iteration 0: 176.790176 \n",
      "Training loss at iteration 25: 87.236075 \n",
      "Training loss at iteration 50: 86.909692 \n",
      "--------------\n",
      "Starting epoch:  2\n",
      "Training loss at iteration 0: 159.679413 \n",
      "Training loss at iteration 25: 78.235770 \n",
      "Training loss at iteration 50: 77.803506 \n",
      "--------------\n",
      "Total number of training steps:  180\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "\n",
    "number_of_epochs = 3\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    # Initialize variables\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "    \n",
    "    # Feed iterator with training data\n",
    "    training_handle = sess.run(training_iterator.string_handle())\n",
    "       \n",
    "    for epoch in range(number_of_epochs):\n",
    "        \n",
    "        # Tell iterator to go to beginning of dataset\n",
    "        sess.run(training_iterator.initializer)\n",
    "\n",
    "        # Initialize variables\n",
    "        losses = []\n",
    "        iteration = 0\n",
    "        \n",
    "        print (\"Starting epoch: \", epoch)\n",
    "        \n",
    "        # iterate over the training dataset and train\n",
    "        while True:\n",
    "            try:\n",
    "                loss,_ = sess.run([loss_op, train_op], feed_dict={handle: training_handle})\n",
    "                losses.append(loss)\n",
    "                if iteration % 25 == 0:                  \n",
    "                    print(\"Training loss at iteration %i: %f \" % (iteration, sum(losses)/len(losses)))\n",
    "                iteration += 1  \n",
    "            except tf.errors.OutOfRangeError:\n",
    "                print(\"--------------\")\n",
    "                break\n",
    "                \n",
    "    print(\"Total number of training steps: \", sess.run(global_step))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Validation cost during training\n",
    "\n",
    "Now, let's see how we can enhance the previous code to compute validation loss periodically during training.\n",
    "\n",
    "### Create validation dataset\n",
    "\n",
    "I will not comment this part. It is just about creating a fake validation dataset. You should replace this to create your own real validation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create random files containing raw matrix of shape 3x3\n",
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        # Create matrix with fake data\n",
    "        matrix = np.zeros((height,width)) + i + (j*10) + 2\n",
    "        # Save matrix as raw float32 file\n",
    "        matrix.astype('float32').tofile('data/validation_' + str(i) + '_' + str(j) + '.raw')\n",
    "        \n",
    "# Get the filenames lists\n",
    "val_filenames0 = ['data/validation_0_0.raw', 'data/validation_0_1.raw']\n",
    "val_filenames1 = ['data/validation_1_0.raw', 'data/validation_1_1.raw']\n",
    "\n",
    "# Create tensorflow constant containing the filenames\n",
    "tf_val_filenames0 = tf.constant(val_filenames0)\n",
    "tf_val_filenames1 = tf.constant(val_filenames1)\n",
    "\n",
    "# Create labels dataset\n",
    "val_labels = tf.constant([19, 21])\n",
    "\n",
    "# Create dataset containing filenames and labels\n",
    "val_dataset = tfdata.Dataset.from_tensor_slices((tf_val_filenames0, tf_val_filenames1, labels))\n",
    "\n",
    "# Use our _parse_data function to read files and decode data\n",
    "val_dataset = val_dataset.map(_parse_data)\n",
    "\n",
    "# Set batch size\n",
    "val_dataset = val_dataset.batch(1)\n",
    "\n",
    "# create TensorFlow Iterator object\n",
    "val_iterator = val_dataset.make_initializable_iterator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute validation loss\n",
    "\n",
    "We define a *run_validation* function. This function iterates over a validation dataset, computes losses for each batch, and returns the mean of the losses.\n",
    "\n",
    "Many loss functions can be aggregated by averaging losses over batches. But for some loss functions, you might have to change the way you make the aggregation. For instance, for RMSE loss, you would have to square the losses before taking the average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_validation(loss_op, my_handle, sess):  # Called inside train_loop()\n",
    "    # iterate over the dataset\n",
    "    losses = []\n",
    "    while True:\n",
    "        try:\n",
    "            losses.append(sess.run(loss_op, feed_dict={handle: my_handle}))\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            break\n",
    "    \n",
    "    # Return the mean of the losses\n",
    "    return sum(losses) / len(losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training!\n",
    "\n",
    "The training process is similar to above. We have added computation of validation loss at every epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch:  0\n",
      "Training loss at iteration 0: 233.436707 \n",
      "Training loss at iteration 25: 338.397340 \n",
      "Training loss at iteration 50: 229.738215 \n",
      "***** Summary for epoch 0 - 60 iterations *****\n",
      "Training loss: 207.794716 \n",
      "Validation loss:  52.845749855\n",
      "---------------\n",
      "Starting epoch:  1\n",
      "Training loss at iteration 0: 184.263763 \n",
      "Training loss at iteration 25: 91.353576 \n",
      "Training loss at iteration 50: 91.646374 \n",
      "***** Summary for epoch 1 - 60 iterations *****\n",
      "Training loss: 89.443359 \n",
      "Validation loss:  49.0337572098\n",
      "---------------\n",
      "Starting epoch:  2\n",
      "Training loss at iteration 0: 171.765945 \n",
      "Training loss at iteration 25: 84.660109 \n",
      "Training loss at iteration 50: 84.825213 \n",
      "***** Summary for epoch 2 - 60 iterations *****\n",
      "Training loss: 82.743513 \n",
      "Validation loss:  46.7505588531\n",
      "---------------\n",
      "Total number of training steps:  180\n"
     ]
    }
   ],
   "source": [
    "# Train and validate\n",
    "\n",
    "number_of_epochs = 3\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    # Initialize variables\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "    \n",
    "    # Create training data and validation data handles\n",
    "    training_handle = sess.run(training_iterator.string_handle())\n",
    "    validation_handle = sess.run(val_iterator.string_handle())\n",
    "    \n",
    "    for epoch in range(number_of_epochs):\n",
    "        \n",
    "        # Tell iterator to go to beginning of dataset\n",
    "        sess.run(training_iterator.initializer)\n",
    "    \n",
    "        # Initialize variables\n",
    "        \n",
    "        losses = []\n",
    "        iteration = 0\n",
    "        \n",
    "        print (\"Starting epoch: \", epoch)\n",
    "    \n",
    "        # iterate over the training dataset and train\n",
    "        while True:\n",
    "            try:\n",
    "                loss,_ = sess.run([loss_op, train_op], feed_dict={handle: training_handle})\n",
    "                losses.append(loss)\n",
    "                if iteration % 25 == 0:                  \n",
    "                    print(\"Training loss at iteration %i: %f \" % (iteration, sum(losses)/len(losses)))\n",
    "                iteration += 1                    \n",
    "            except tf.errors.OutOfRangeError:\n",
    "                print(\"***** Summary for epoch %i - %i iterations *****\" % (epoch, iteration))\n",
    "                print(\"Training loss: %f \" % (sum(losses)/len(losses)))\n",
    "                break              \n",
    "\n",
    "        # Tell validation iterator to go to beginning of dataset\n",
    "        sess.run(val_iterator.initializer)\n",
    "\n",
    "        # run validation\n",
    "        print(\"Validation loss: \", run_validation(loss_op, validation_handle, sess))\n",
    "        print(\"---------------\")\n",
    "        \n",
    "    print(\"Total number of training steps: \", sess.run(global_step))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Monitor training with TensorBoard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to train your model accurately, you will need to monitor the training process.\n",
    "\n",
    "TensorFlow provides a nice web interface for monitoring, called TensorBoard. See here: https://www.tensorflow.org/get_started/summaries_and_tensorboard\n",
    "\n",
    "We are going to add a few lines in our previous code, in order to write logs for TensorBoard. We will log the following data:\n",
    "* Training loss every 25 steps\n",
    "* Average of training loss over each epoch\n",
    "* Average of validation loss after each epoch\n",
    "\n",
    "The logs will be written to `./logs` directory. To launch TensorBoard and visualize logs, open a terminal, cd to this notebook directory, and enter:\n",
    "```bash\n",
    "tensorboard --logdir=logs\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch:  0\n",
      "Training loss at iteration 0: 219.763962 \n",
      "Training loss at iteration 25: 126.445244 \n",
      "Training loss at iteration 50: 113.693544 \n",
      "***** Summary for epoch 0 - 60 iterations *****\n",
      "Training loss: 108.995984 \n",
      "Validation loss:  50.5948963165\n",
      "---------------\n",
      "Starting epoch:  1\n",
      "Training loss at iteration 0: 183.250961 \n",
      "Training loss at iteration 25: 89.351678 \n",
      "Training loss at iteration 50: 88.419110 \n",
      "***** Summary for epoch 1 - 60 iterations *****\n",
      "Training loss: 85.910143 \n",
      "Validation loss:  45.4535541534\n",
      "---------------\n",
      "Starting epoch:  2\n",
      "Training loss at iteration 0: 158.864838 \n",
      "Training loss at iteration 25: 76.967960 \n",
      "Training loss at iteration 50: 75.475657 \n",
      "***** Summary for epoch 2 - 60 iterations *****\n",
      "Training loss: 73.094713 \n",
      "Validation loss:  35.7089834213\n",
      "---------------\n",
      "Total number of training steps:  180\n"
     ]
    }
   ],
   "source": [
    "# Train, validate and write logs for TensorBoard\n",
    "\n",
    "number_of_epochs = 3\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    # Initialize variables\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "    \n",
    "    # Create training data and validation data handles\n",
    "    training_handle = sess.run(training_iterator.string_handle())\n",
    "    validation_handle = sess.run(val_iterator.string_handle())\n",
    "    \n",
    "    # Logs for TensorBoard: create summary writers\n",
    "    train_writer = tf.summary.FileWriter('logs/train',\n",
    "                                      sess.graph)\n",
    "    val_writer = tf.summary.FileWriter('logs/val')\n",
    "\n",
    "    \n",
    "    # Create a summary to monitor loss\n",
    "    tf.summary.scalar(\"loss\", loss_op)\n",
    "    # In this example, we have only one summary op, but if you have many, it is convenient to merge them in one op\n",
    "    merged_summary_op = tf.summary.merge_all()\n",
    "    \n",
    "    for epoch in range(number_of_epochs):\n",
    "        \n",
    "        # Tell iterator to go to beginning of dataset\n",
    "        sess.run(training_iterator.initializer)\n",
    "    \n",
    "        # Initialize variables\n",
    "        \n",
    "        losses = []\n",
    "        iteration = 0\n",
    "        \n",
    "        print (\"Starting epoch: \", epoch)\n",
    "    \n",
    "        # iterate over the training dataset and train\n",
    "        while True:\n",
    "            try:\n",
    "                global_iteration, merged_summary, loss,_ = sess.run([global_step, merged_summary_op, loss_op, train_op], feed_dict={handle: training_handle})\n",
    "                losses.append(loss)\n",
    "                if iteration % 25 == 0:                  \n",
    "                    print(\"Training loss at iteration %i: %f \" % (iteration, sum(losses)/len(losses)))\n",
    "                    # Log loss for tensorboard\n",
    "                    train_writer.add_summary(merged_summary, global_iteration)\n",
    "                iteration += 1\n",
    "            except tf.errors.OutOfRangeError:\n",
    "                print(\"***** Summary for epoch %i - %i iterations *****\" % (epoch, iteration))\n",
    "                loss_over_epoch = sum(losses)/len(losses)\n",
    "                print(\"Training loss: %f \" % loss_over_epoch)\n",
    "                \n",
    "                # Log average loss for this epoch (for TensorBoard)\n",
    "                avg_loss_summary = tf.Summary()\n",
    "                avg_loss_summary.value.add(tag=\"epochLoss\", simple_value=loss_over_epoch)\n",
    "                train_writer.add_summary(avg_loss_summary, global_iteration)                \n",
    "                break              \n",
    "\n",
    "        # Tell validation iterator to go to beginning of dataset\n",
    "        sess.run(val_iterator.initializer)\n",
    "\n",
    "        # run validation\n",
    "        validation_loss = run_validation(loss_op, validation_handle, sess)\n",
    "        print(\"Validation loss: \", validation_loss)\n",
    "        print(\"---------------\")\n",
    "        \n",
    "        # Log validation loss for TensorBoard\n",
    "        val_loss_summary = tf.Summary()\n",
    "        val_loss_summary.value.add(tag=\"valLoss\", simple_value=validation_loss)\n",
    "        val_writer.add_summary(val_loss_summary, global_iteration)\n",
    "        \n",
    "    print(\"Total number of training steps: \", sess.run(global_step))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving and restoring the model\n",
    "Training a model can take a lot of time. It is wise to save periodically the result of your training, to be able to restore it whenever you need.\n",
    "\n",
    "This is what we are going to add to our code:\n",
    "* Before we start training, check if we already have a saved model. If so, load it.\n",
    "* During training, save the model periodically.\n",
    "* At the end of training, save the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from saved_model\\model.ckpt-1800\n",
      "Starting epoch:  0\n",
      "Training loss at iteration 0: 0.000003 \n",
      "Training loss at iteration 25: 0.000001 \n",
      "Training loss at iteration 50: 0.000001 \n",
      "***** Summary for epoch 0 - 60 iterations *****\n",
      "Training loss: 0.000001 \n",
      "Validation loss:  3.89901578426\n",
      "---------------\n",
      "Starting epoch:  1\n",
      "Training loss at iteration 0: 0.000001 \n",
      "Training loss at iteration 25: 0.000000 \n",
      "-----> Model saved in file: saved_model/model.ckpt-1900\n",
      "Training loss at iteration 50: 0.000000 \n",
      "***** Summary for epoch 1 - 60 iterations *****\n",
      "Training loss: 0.000000 \n",
      "Validation loss:  3.89899325371\n",
      "---------------\n",
      "Starting epoch:  2\n",
      "Training loss at iteration 0: 0.000000 \n",
      "Training loss at iteration 25: 0.000000 \n",
      "Training loss at iteration 50: 0.000000 \n",
      "***** Summary for epoch 2 - 60 iterations *****\n",
      "Training loss: 0.000000 \n",
      "Validation loss:  3.89900457859\n",
      "---------------\n",
      "Total number of training steps:  1980\n",
      "-----> Model saved in file: saved_model/model.ckpt-1980\n"
     ]
    }
   ],
   "source": [
    "# Train, validate and write logs for TensorBoard\n",
    "\n",
    "number_of_epochs = 3\n",
    "\n",
    "saving_dir = 'saved_model'\n",
    "\n",
    "# Add ops to save and restore the model\n",
    "saver = tf.train.Saver(max_to_keep=20)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    # Check if model has been previously saved\n",
    "    if os.path.isfile(saving_dir + '/checkpoint'):\n",
    "        # Restore model from file\n",
    "        saver.restore(sess, tf.train.latest_checkpoint(saving_dir))\n",
    "    else:\n",
    "        # Initialize variables\n",
    "        init = tf.global_variables_initializer()\n",
    "        sess.run(init)\n",
    "    \n",
    "    # Restore the previously saved model\n",
    "    \n",
    "    # Create training data and validation data handles\n",
    "    training_handle = sess.run(training_iterator.string_handle())\n",
    "    validation_handle = sess.run(val_iterator.string_handle())\n",
    "    \n",
    "    # Logs for TensorBoard: create summary writers\n",
    "    train_writer = tf.summary.FileWriter('logs/train',\n",
    "                                      sess.graph)\n",
    "    val_writer = tf.summary.FileWriter('logs/val')\n",
    "\n",
    "    \n",
    "    # Create a summary to monitor loss\n",
    "    tf.summary.scalar(\"loss\", loss_op)\n",
    "    # In this example, we have only one summary op, but if you have many, it is convenient to merge them in one op\n",
    "    merged_summary_op = tf.summary.merge_all()\n",
    "    \n",
    "    for epoch in range(number_of_epochs):\n",
    "        \n",
    "        # Tell iterator to go to beginning of dataset\n",
    "        sess.run(training_iterator.initializer)\n",
    "    \n",
    "        # Initialize variables\n",
    "        \n",
    "        losses = []\n",
    "        iteration = 0\n",
    "        \n",
    "        print (\"Starting epoch: \", epoch)\n",
    "    \n",
    "        # iterate over the training dataset and train\n",
    "        while True:\n",
    "            try:\n",
    "                global_iteration, merged_summary, loss,_ = sess.run([global_step, merged_summary_op, loss_op, train_op], feed_dict={handle: training_handle})\n",
    "                losses.append(loss)\n",
    "                if iteration % 25 == 0:                  \n",
    "                    print(\"Training loss at iteration %i: %f \" % (iteration, sum(losses)/len(losses)))\n",
    "                    # Log loss for tensorboard\n",
    "                    train_writer.add_summary(merged_summary, global_iteration)\n",
    "                iteration += 1\n",
    "\n",
    "                # Save model periodically\n",
    "                if global_iteration % 100 == 0:\n",
    "                    save_path = saver.save(sess, saving_dir + \"/model.ckpt\", global_step=global_iteration)\n",
    "                    print(\"-----> Model saved in file: %s\" % save_path)\n",
    "\n",
    "            except tf.errors.OutOfRangeError:\n",
    "                print(\"***** Summary for epoch %i - %i iterations *****\" % (epoch, iteration))\n",
    "                loss_over_epoch = sum(losses)/len(losses)\n",
    "                print(\"Training loss: %f \" % loss_over_epoch)\n",
    "                \n",
    "                # Log average loss for this epoch (for TensorBoard)\n",
    "                avg_loss_summary = tf.Summary()\n",
    "                avg_loss_summary.value.add(tag=\"epochLoss\", simple_value=loss_over_epoch)\n",
    "                train_writer.add_summary(avg_loss_summary, global_iteration)                \n",
    "                break              \n",
    "\n",
    "        # Tell validation iterator to go to beginning of dataset\n",
    "        sess.run(val_iterator.initializer)\n",
    "\n",
    "        # run validation\n",
    "        validation_loss = run_validation(loss_op, validation_handle, sess)\n",
    "        print(\"Validation loss: \", validation_loss)\n",
    "        print(\"---------------\")\n",
    "        \n",
    "        # Log validation loss for TensorBoard\n",
    "        val_loss_summary = tf.Summary()\n",
    "        val_loss_summary.value.add(tag=\"valLoss\", simple_value=validation_loss)\n",
    "        val_writer.add_summary(val_loss_summary, global_iteration)\n",
    "\n",
    "    print(\"Total number of training steps: \", sess.run(global_step))\n",
    "    \n",
    "    # Save the model at end of training\n",
    "    save_path = saver.save(sess, saving_dir + \"/model.ckpt\", global_step=sess.run(global_step))\n",
    "    print(\"-----> Model saved in file: %s\" % save_path)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make a prediction\n",
    "\n",
    "Now we have a trained model, we can make predictions.\n",
    "\n",
    "### Create input dataset\n",
    "\n",
    "As we have used an iterator to feed our model, we need to put our examples in an iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For illustration purpose, we will take our validation examples to feed the predictor.\n",
    "# In real life, you will of course use some other data.\n",
    "\n",
    "# Get the filenames lists\n",
    "input_filenames0 = ['data/validation_0_0.raw', 'data/validation_0_1.raw']\n",
    "input_filenames1 = ['data/validation_1_0.raw', 'data/validation_1_1.raw']\n",
    "\n",
    "# Create tensorflow constant containing the filenames\n",
    "tf_input_filenames0 = tf.constant(input_filenames0)\n",
    "tf_input_filenames1 = tf.constant(input_filenames1)\n",
    "\n",
    "# Create labels. These data will not be used, but we need its type and shape to match the training dataset.\n",
    "predict_labels = tf.constant([0,0])\n",
    "\n",
    "# Create dataset containing filenames and labels\n",
    "input_dataset = tfdata.Dataset.from_tensor_slices((tf_input_filenames0, tf_input_filenames1, predict_labels))\n",
    "\n",
    "# Use our _parse_data function to read files and decode data\n",
    "input_dataset = input_dataset.map(_parse_data)\n",
    "\n",
    "# Set batch size\n",
    "input_dataset = input_dataset.batch(1)\n",
    "\n",
    "# create TensorFlow Iterator object\n",
    "input_iterator = input_dataset.make_initializable_iterator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make the prediction\n",
    "\n",
    "Here, we will load the model from file, and then feed our iterator into the model to make the prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from saved_model\\model.ckpt-1980\n",
      "Prediction 0:  16.931\n",
      "Prediction 1:  27.0173\n",
      "We have made 2 predictions.\n"
     ]
    }
   ],
   "source": [
    "saving_dir = 'saved_model'\n",
    "\n",
    "# Add ops to save and restore the model\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "        \n",
    "    # Create iterator handle with input data\n",
    "    input_handle = sess.run(input_iterator.string_handle())\n",
    "       \n",
    "    # Check if model has been previously saved\n",
    "    if os.path.isfile(saving_dir + '/checkpoint'):\n",
    "        # Restore model\n",
    "        saver.restore(sess, tf.train.latest_checkpoint(saving_dir))\n",
    "    else:\n",
    "        print(\"Unable to find model file.\")\n",
    "\n",
    "    # Initialize a list that will contain our predictions\n",
    "    predictions=[]\n",
    "        \n",
    "    # Tell iterator to go to beginning of dataset\n",
    "    sess.run(input_iterator.initializer)\n",
    "\n",
    "    i = 0\n",
    "\n",
    "    # iterate over the input dataset and make prediction\n",
    "    while True:\n",
    "        try:\n",
    "            prediction = sess.run([predict_op], feed_dict={handle: input_handle})\n",
    "            predictions.append(prediction)\n",
    "            print(\"Prediction %i: \" %i , prediction[0][0][0])\n",
    "            i += 1\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            break\n",
    "                \n",
    "    print(\"We have made %i predictions.\" % (len(predictions)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
