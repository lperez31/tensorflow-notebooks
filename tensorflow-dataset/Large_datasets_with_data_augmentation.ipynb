{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow input pipeline: large datasets and data augmentation\n",
    "\n",
    "## Introduction\n",
    "\n",
    "The new high level Dataset API makes it quite easy to deal with large datasets. No need to bother with queues anymore.\n",
    "\n",
    "This notebook gives a short example of how to use the Dataset API for:\n",
    "\n",
    "1. loading multiple files for each input example,\n",
    "2. data augmentation.\n",
    "\n",
    "Loading multiple files for each input example can be needed for several applications:\n",
    "\n",
    "* dealing with image sequences,\n",
    "* object detection with multiple cameras,\n",
    "* etc.\n",
    "\n",
    "Data augmentation is a classical problem in deep learning. However, the Dataset API documentation does not give any hint about how to achieve data augmentation. I am not sure that the method used below is the best for this task. If you find a better way, don't hesitate to tell me!\n",
    "\n",
    "__Note on tensorflow version:__\n",
    "\n",
    "> In tensorflow v1.4, *tf.contrib.data* has been integrated into *tf.data*. Please use appropriate import according to your tensorflow version. See comment in code below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's go!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# For tensorflow 1.3\n",
    "#from tensorflow.contrib import data as tfdata\n",
    "\n",
    "# For tensorflow 1.4 and above\n",
    "from tensorflow import data as tfdata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create some dummy data\n",
    "\n",
    "We are going to create some dummy data files. In a real application, use your own data files. Here, we will create 4 files: two groups of two files. Each file will contain a 3x3 matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "height = 3\n",
    "width = 3\n",
    "\n",
    "# Create random files containing raw matrix of shape 3x3\n",
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        # Create matrix with fake data\n",
    "        matrix = np.zeros((height,width)) + i + (j*10)\n",
    "        # Save matrix as raw float32 file\n",
    "        matrix.astype('float32').tofile('data/file_' + str(i) + '_' + str(j) + '.raw')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at one of our dummy data files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 11.  11.  11.]\n",
      " [ 11.  11.  11.]\n",
      " [ 11.  11.  11.]]\n"
     ]
    }
   ],
   "source": [
    "# Print one of the generated files for checking\n",
    "matrix = np.fromfile('data/file_1_1.raw', dtype=np.float32)\n",
    "matrix = matrix.reshape((height,width))\n",
    "print(matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, our dummy data file looks good."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create parser\n",
    "\n",
    "We will need a parser to read our examples from the data files.\n",
    "\n",
    "Here, each example will be created by stacking data coming from two different files and a label. Therefore, the parser arguments will be the two filenames and the label. It will return two tensors containing the example and the label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create parser\n",
    "# Args: filenames\n",
    "# Returns: tensor containing read and decoded element\n",
    "def _parse_data(filename0, filename1, label):\n",
    "    # Read and decode first file\n",
    "    matrix0 = tf.read_file(filename0)\n",
    "    matrix0 = tf.decode_raw(matrix0, out_type=tf.float32)\n",
    "    matrix0 = tf.reshape(matrix0, [height,width])\n",
    "    \n",
    "    # Read and decode second file\n",
    "    matrix1 = tf.read_file(filename1)\n",
    "    matrix1 = tf.decode_raw(matrix1, out_type=tf.float32)\n",
    "    matrix1 = tf.reshape(matrix1, [height,width])\n",
    "    \n",
    "    # Stack the two elements together\n",
    "    X = tf.stack([matrix0, matrix1])\n",
    "    \n",
    "    # Get label (you could implement more complex logic here if needed)\n",
    "    y = tf.reshape(label, [1])\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create data augmentation function\n",
    "\n",
    "We will need a function to implement the data augmentation logic.\n",
    "\n",
    "This function takes one example (X,y), and returns a dataset with two examples: the original example, and a second one generated on the fly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Data augmentation function: create several examples from one example\n",
    "# Args: One example X,y\n",
    "# Returns: Dataset containing several examples, after data augmentation\n",
    "def _data_augment(X,y):\n",
    "    # Generate new data from example X\n",
    "    X0 = X\n",
    "    X1 = -X # Dummy data augmentation, but we could use any transformation we need. We could also generate more examples.\n",
    "    X_augmented = tf.stack([X0,X1])\n",
    "    \n",
    "    # Repeat y\n",
    "    y_augmented = tf.stack([y,y])\n",
    "    \n",
    "    dataset = tfdata.Dataset.from_tensor_slices((X_augmented, y_augmented))\n",
    "        \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### It's almost done\n",
    "\n",
    "Now that we have implemented our parsing and data augmentation logic, we can create the dataset.\n",
    "\n",
    "As you can see, with the TensorFlow Dataset API, it is really easy! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "####################################################################\n",
    "# Create dataset from files\n",
    "####################################################################\n",
    "\n",
    "# Get the filenames lists\n",
    "filenames0 = ['data/file_0_0.raw', 'data/file_0_1.raw']\n",
    "filenames1 = ['data/file_1_0.raw', 'data/file_1_1.raw']\n",
    "\n",
    "# Create tensorflow constant containing the filenames\n",
    "tf_filenames0 = tf.constant(filenames0)\n",
    "tf_filenames1 = tf.constant(filenames1)\n",
    "\n",
    "# Create labels dataset\n",
    "labels = tf.constant([15, 25])\n",
    "\n",
    "# Create dataset containing filenames and labels\n",
    "dataset = tfdata.Dataset.from_tensor_slices((tf_filenames0, tf_filenames1, labels))\n",
    "\n",
    "# Use our _parse_data function to read files and decode data\n",
    "dataset = dataset.map(_parse_data)\n",
    "\n",
    "#####################################################################\n",
    "# Data augmentation\n",
    "#####################################################################\n",
    "\n",
    "# Apply data augmentation\n",
    "dataset = dataset.interleave(_data_augment, cycle_length=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print the dataset\n",
    "\n",
    "We can now print the dataset to check that everything is working."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([[[ 0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.]],\n",
      "\n",
      "       [[ 1.,  1.,  1.],\n",
      "        [ 1.,  1.,  1.],\n",
      "        [ 1.,  1.,  1.]]], dtype=float32), array([15]))\n",
      "(array([[[-0., -0., -0.],\n",
      "        [-0., -0., -0.],\n",
      "        [-0., -0., -0.]],\n",
      "\n",
      "       [[-1., -1., -1.],\n",
      "        [-1., -1., -1.],\n",
      "        [-1., -1., -1.]]], dtype=float32), array([15]))\n",
      "(array([[[ 10.,  10.,  10.],\n",
      "        [ 10.,  10.,  10.],\n",
      "        [ 10.,  10.,  10.]],\n",
      "\n",
      "       [[ 11.,  11.,  11.],\n",
      "        [ 11.,  11.,  11.],\n",
      "        [ 11.,  11.,  11.]]], dtype=float32), array([25]))\n",
      "(array([[[-10., -10., -10.],\n",
      "        [-10., -10., -10.],\n",
      "        [-10., -10., -10.]],\n",
      "\n",
      "       [[-11., -11., -11.],\n",
      "        [-11., -11., -11.],\n",
      "        [-11., -11., -11.]]], dtype=float32), array([25]))\n",
      "End of dataset\n"
     ]
    }
   ],
   "source": [
    "#####################################################################\n",
    "# Print the dataset\n",
    "#####################################################################\n",
    "\n",
    "# create TensorFlow Iterator object\n",
    "iterator = dataset.make_one_shot_iterator()\n",
    "next_element = iterator.get_next()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "\n",
    "    # iterate over the dataset\n",
    "    while True:\n",
    "        try:\n",
    "            elem = sess.run(next_element)\n",
    "            print(elem)\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            print(\"End of dataset\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### It is working!\n",
    "\n",
    "We can see that parsing and data augmentation work well:\n",
    "\n",
    "* each example contains:\n",
    "  * two matrices coming from two different files\n",
    "  * a label\n",
    "* after each example, there is an augmented example (-X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's train our model now\n",
    "\n",
    "First we have to create a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a small dummy model\n",
    "def model_function(input_data):\n",
    "    flattened = tf.contrib.layers.flatten(input_data)\n",
    "    output = tf.layers.dense(flattened, 1)\n",
    "    return output\n",
    "\n",
    "# Create a loss function    \n",
    "def loss_function(example, label):\n",
    "    output = model_function(example)\n",
    "    return tf.losses.mean_squared_error(label, output)\n",
    "\n",
    "t_input_data = tf.placeholder(tf.float32, shape=[None, 2, 3, 3])\n",
    "t_label = tf.placeholder(tf.float32, shape=[None, 1])\n",
    "t_output = model_function(t_input_data)\n",
    "t_loss = tf.losses.mean_squared_error(t_label, t_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will do the following:\n",
    "\n",
    "* Prepare the dataset for training\n",
    "  * Repeat the dataset for number of epochs\n",
    "  * Set the batch size\n",
    "  * Create an iterator on the batched dataset\n",
    "  * Compute the number of steps per epoch\n",
    "* Create a global_step variable. This variable will count the number of training steps.\n",
    "* Create the loss op and the train op."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Repeat dataset for number of epochs\n",
    "number_of_epochs = 30\n",
    "batch_dataset = dataset.repeat(number_of_epochs)\n",
    "\n",
    "# Set batch size\n",
    "batch_dataset = batch_dataset.batch(2)\n",
    "\n",
    "# create TensorFlow Iterator object\n",
    "iterator = batch_dataset.make_one_shot_iterator()\n",
    "\n",
    "# Get example, compute loss and create optimizer\n",
    "next_batch_examples, next_batch_labels = iterator.get_next()\n",
    "\n",
    "# Create global_step variable, to store the global training step\n",
    "global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "\n",
    "loss = loss_function(next_batch_examples, next_batch_labels)\n",
    "train_op = tf.train.GradientDescentOptimizer(1e-3).minimize(loss, global_step=global_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### That's all. We can train!\n",
    "\n",
    "We will train the model by using *tf.train.MonitoredTrainingSession* instead of *tf.Session*. The MonitoredTrainingSession has builtin hooks that automate some common tasks:\n",
    "\n",
    "* Save the model periodically during training,\n",
    "* Restore previous training before starting a new training session,\n",
    "* Create logs for tensorboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from ./saved_training\\model.ckpt-168\n",
      "INFO:tensorflow:Saving checkpoints for 169 into ./saved_training\\model.ckpt.\n",
      "INFO:tensorflow:Saving checkpoints for 228 into ./saved_training\\model.ckpt.\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "\n",
    "# MonitoredTrainingSession example\n",
    "with tf.train.MonitoredTrainingSession(checkpoint_dir='./saved_training',\n",
    "                                       save_checkpoint_secs=10,\n",
    "                                       save_summaries_steps=2) as sess:\n",
    "    while not sess.should_stop():\n",
    "        sess.run(train_op)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Validation cost during training\n",
    "\n",
    "Now, let's see how we can enhance the previous code to compute validation loss periodically during training.\n",
    "\n",
    "### Create validation dataset\n",
    "\n",
    "I will not comment this part. It is just about creating a fake validation dataset. You should replace this by creating your own real validation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create random files containing raw matrix of shape 3x3\n",
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        # Create matrix with fake data\n",
    "        matrix = np.zeros((height,width)) + i + (j*10) + 2\n",
    "        # Save matrix as raw float32 file\n",
    "        matrix.astype('float32').tofile('data/validation_' + str(i) + '_' + str(j) + '.raw')\n",
    "        \n",
    "# Get the filenames lists\n",
    "val_filenames0 = ['data/validation_0_0.raw', 'data/validation_0_1.raw']\n",
    "val_filenames1 = ['data/validation_1_0.raw', 'data/validation_1_1.raw']\n",
    "\n",
    "# Create tensorflow constant containing the filenames\n",
    "tf_val_filenames0 = tf.constant(val_filenames0)\n",
    "tf_val_filenames1 = tf.constant(val_filenames1)\n",
    "\n",
    "# Create labels dataset\n",
    "val_labels = tf.constant([19, 21])\n",
    "\n",
    "# Create dataset containing filenames and labels\n",
    "val_dataset = tfdata.Dataset.from_tensor_slices((tf_filenames0, tf_filenames1, labels))\n",
    "\n",
    "# Use our _parse_data function to read files and decode data\n",
    "val_dataset = val_dataset.map(_parse_data)\n",
    "\n",
    "# Set batch size\n",
    "val_dataset = val_dataset.batch(2)\n",
    "\n",
    "# create TensorFlow Iterator object\n",
    "val_iterator = val_dataset.make_one_shot_iterator()\n",
    "\n",
    "# Get example, compute loss and create optimizer\n",
    "next_val_examples, next_val_labels = iterator.get_next()\n",
    "\n",
    "# Create validation loss op\n",
    "val_loss = loss_function(next_val_examples, next_val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_validation(op, sess):  # Called inside train_loop()\n",
    "    # iterate over the dataset\n",
    "    while True:\n",
    "        try:\n",
    "            loss = sess.run(op)\n",
    "            print(loss)\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            print(\"End of dataset\")\n",
    "            break\n",
    "    \n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from ./saved_training\\model.ckpt-288\n",
      "INFO:tensorflow:Saving checkpoints for 289 into ./saved_training\\model.ckpt.\n"
     ]
    },
    {
     "ename": "FailedPreconditionError",
     "evalue": "Attempting to use uninitialized value dense_3/bias\n\t [[Node: dense_3/bias/read = Identity[T=DT_FLOAT, _class=[\"loc:@dense_3/bias\"], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](dense_3/bias)]]\n\nCaused by op 'dense_3/bias/read', defined at:\n  File \"c:\\users\\perezl\\appdata\\local\\programs\\python\\python36\\lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"c:\\users\\perezl\\appdata\\local\\programs\\python\\python36\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"c:\\users\\perezl\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"c:\\users\\perezl\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\traitlets\\config\\application.py\", line 658, in launch_instance\n    app.start()\n  File \"c:\\users\\perezl\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"c:\\users\\perezl\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\zmq\\eventloop\\ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"c:\\users\\perezl\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tornado\\ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"c:\\users\\perezl\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tornado\\stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"c:\\users\\perezl\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"c:\\users\\perezl\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"c:\\users\\perezl\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"c:\\users\\perezl\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tornado\\stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"c:\\users\\perezl\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"c:\\users\\perezl\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"c:\\users\\perezl\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"c:\\users\\perezl\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"c:\\users\\perezl\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"c:\\users\\perezl\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2683, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"c:\\users\\perezl\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2787, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"c:\\users\\perezl\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2847, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-14-13748a28053d>\", line 38, in <module>\n    val_loss = loss_function(next_val_examples, next_val_labels)\n  File \"<ipython-input-8-15ff9ffa13e5>\", line 9, in loss_function\n    output = model_function(example)\n  File \"<ipython-input-8-15ff9ffa13e5>\", line 4, in model_function\n    output = tf.layers.dense(flattened, 1)\n  File \"c:\\users\\perezl\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\layers\\core.py\", line 250, in dense\n    return layer.apply(inputs)\n  File \"c:\\users\\perezl\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\layers\\base.py\", line 671, in apply\n    return self.__call__(inputs, *args, **kwargs)\n  File \"c:\\users\\perezl\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\layers\\base.py\", line 559, in __call__\n    self.build(input_shapes[0])\n  File \"c:\\users\\perezl\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\layers\\core.py\", line 145, in build\n    trainable=True)\n  File \"c:\\users\\perezl\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\layers\\base.py\", line 458, in add_variable\n    trainable=trainable and self.trainable)\n  File \"c:\\users\\perezl\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\", line 1203, in get_variable\n    constraint=constraint)\n  File \"c:\\users\\perezl\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\", line 1092, in get_variable\n    constraint=constraint)\n  File \"c:\\users\\perezl\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\", line 425, in get_variable\n    constraint=constraint)\n  File \"c:\\users\\perezl\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\", line 394, in _true_getter\n    use_resource=use_resource, constraint=constraint)\n  File \"c:\\users\\perezl\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\", line 805, in _get_single_variable\n    constraint=constraint)\n  File \"c:\\users\\perezl\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py\", line 213, in __init__\n    constraint=constraint)\n  File \"c:\\users\\perezl\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py\", line 356, in _init_from_args\n    self._snapshot = array_ops.identity(self._variable, name=\"read\")\n  File \"c:\\users\\perezl\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py\", line 125, in identity\n    return gen_array_ops.identity(input, name=name)\n  File \"c:\\users\\perezl\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\gen_array_ops.py\", line 2070, in identity\n    \"Identity\", input=input, name=name)\n  File \"c:\\users\\perezl\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"c:\\users\\perezl\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 2956, in create_op\n    op_def=op_def)\n  File \"c:\\users\\perezl\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1470, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nFailedPreconditionError (see above for traceback): Attempting to use uninitialized value dense_3/bias\n\t [[Node: dense_3/bias/read = Identity[T=DT_FLOAT, _class=[\"loc:@dense_3/bias\"], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](dense_3/bias)]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFailedPreconditionError\u001b[0m                   Traceback (most recent call last)",
      "\u001b[1;32mc:\\users\\perezl\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1322\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1323\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1324\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\perezl\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1302\u001b[1;33m                                    status, run_metadata)\n\u001b[0m\u001b[0;32m   1303\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\perezl\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\framework\\errors_impl.py\u001b[0m in \u001b[0;36m__exit__\u001b[1;34m(self, type_arg, value_arg, traceback_arg)\u001b[0m\n\u001b[0;32m    472\u001b[0m             \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc_api\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 473\u001b[1;33m             c_api.TF_GetCode(self.status.status))\n\u001b[0m\u001b[0;32m    474\u001b[0m     \u001b[1;31m# Delete the underlying status object from memory otherwise it stays alive\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFailedPreconditionError\u001b[0m: Attempting to use uninitialized value dense_3/bias\n\t [[Node: dense_3/bias/read = Identity[T=DT_FLOAT, _class=[\"loc:@dense_3/bias\"], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](dense_3/bias)]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mFailedPreconditionError\u001b[0m                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-e7590ea91bcb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mstep\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mvalidate_every_n_steps\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m             \u001b[1;31m# Compute validation loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m             \u001b[0mrun_validation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mval_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-16-875515284397>\u001b[0m in \u001b[0;36mrun_validation\u001b[1;34m(op, sess)\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\perezl\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\training\\monitored_session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    519\u001b[0m                           \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    520\u001b[0m                           \u001b[0moptions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 521\u001b[1;33m                           run_metadata=run_metadata)\n\u001b[0m\u001b[0;32m    522\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    523\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mshould_stop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\perezl\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\training\\monitored_session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    890\u001b[0m                               \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    891\u001b[0m                               \u001b[0moptions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 892\u001b[1;33m                               run_metadata=run_metadata)\n\u001b[0m\u001b[0;32m    893\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[0m_PREEMPTION_ERRORS\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    894\u001b[0m         logging.info('An error was raised. This may be due to a preemption in '\n",
      "\u001b[1;32mc:\\users\\perezl\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\training\\monitored_session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    965\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0moriginal_exc_info\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    966\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 967\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0moriginal_exc_info\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    968\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    969\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\perezl\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\six.py\u001b[0m in \u001b[0;36mreraise\u001b[1;34m(tp, value, tb)\u001b[0m\n\u001b[0;32m    691\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mtb\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    692\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 693\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    694\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    695\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\perezl\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\training\\monitored_session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    950\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    951\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 952\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    953\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0m_PREEMPTION_ERRORS\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    954\u001b[0m       \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\perezl\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\training\\monitored_session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1022\u001b[0m                                   \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1023\u001b[0m                                   \u001b[0moptions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1024\u001b[1;33m                                   run_metadata=run_metadata)\n\u001b[0m\u001b[0;32m   1025\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1026\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_hooks\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\perezl\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\training\\monitored_session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    825\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    826\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 827\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    828\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    829\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\perezl\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    887\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 889\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    890\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\perezl\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1118\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1120\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1121\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\perezl\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1315\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[1;32m-> 1317\u001b[1;33m                            options, run_metadata)\n\u001b[0m\u001b[0;32m   1318\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1319\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\perezl\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1334\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1335\u001b[0m           \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1336\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1337\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1338\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFailedPreconditionError\u001b[0m: Attempting to use uninitialized value dense_3/bias\n\t [[Node: dense_3/bias/read = Identity[T=DT_FLOAT, _class=[\"loc:@dense_3/bias\"], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](dense_3/bias)]]\n\nCaused by op 'dense_3/bias/read', defined at:\n  File \"c:\\users\\perezl\\appdata\\local\\programs\\python\\python36\\lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"c:\\users\\perezl\\appdata\\local\\programs\\python\\python36\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"c:\\users\\perezl\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"c:\\users\\perezl\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\traitlets\\config\\application.py\", line 658, in launch_instance\n    app.start()\n  File \"c:\\users\\perezl\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"c:\\users\\perezl\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\zmq\\eventloop\\ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"c:\\users\\perezl\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tornado\\ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"c:\\users\\perezl\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tornado\\stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"c:\\users\\perezl\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"c:\\users\\perezl\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"c:\\users\\perezl\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"c:\\users\\perezl\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tornado\\stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"c:\\users\\perezl\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"c:\\users\\perezl\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"c:\\users\\perezl\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"c:\\users\\perezl\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"c:\\users\\perezl\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"c:\\users\\perezl\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2683, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"c:\\users\\perezl\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2787, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"c:\\users\\perezl\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2847, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-14-13748a28053d>\", line 38, in <module>\n    val_loss = loss_function(next_val_examples, next_val_labels)\n  File \"<ipython-input-8-15ff9ffa13e5>\", line 9, in loss_function\n    output = model_function(example)\n  File \"<ipython-input-8-15ff9ffa13e5>\", line 4, in model_function\n    output = tf.layers.dense(flattened, 1)\n  File \"c:\\users\\perezl\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\layers\\core.py\", line 250, in dense\n    return layer.apply(inputs)\n  File \"c:\\users\\perezl\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\layers\\base.py\", line 671, in apply\n    return self.__call__(inputs, *args, **kwargs)\n  File \"c:\\users\\perezl\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\layers\\base.py\", line 559, in __call__\n    self.build(input_shapes[0])\n  File \"c:\\users\\perezl\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\layers\\core.py\", line 145, in build\n    trainable=True)\n  File \"c:\\users\\perezl\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\layers\\base.py\", line 458, in add_variable\n    trainable=trainable and self.trainable)\n  File \"c:\\users\\perezl\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\", line 1203, in get_variable\n    constraint=constraint)\n  File \"c:\\users\\perezl\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\", line 1092, in get_variable\n    constraint=constraint)\n  File \"c:\\users\\perezl\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\", line 425, in get_variable\n    constraint=constraint)\n  File \"c:\\users\\perezl\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\", line 394, in _true_getter\n    use_resource=use_resource, constraint=constraint)\n  File \"c:\\users\\perezl\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\", line 805, in _get_single_variable\n    constraint=constraint)\n  File \"c:\\users\\perezl\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py\", line 213, in __init__\n    constraint=constraint)\n  File \"c:\\users\\perezl\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py\", line 356, in _init_from_args\n    self._snapshot = array_ops.identity(self._variable, name=\"read\")\n  File \"c:\\users\\perezl\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py\", line 125, in identity\n    return gen_array_ops.identity(input, name=name)\n  File \"c:\\users\\perezl\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\gen_array_ops.py\", line 2070, in identity\n    \"Identity\", input=input, name=name)\n  File \"c:\\users\\perezl\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"c:\\users\\perezl\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 2956, in create_op\n    op_def=op_def)\n  File \"c:\\users\\perezl\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1470, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nFailedPreconditionError (see above for traceback): Attempting to use uninitialized value dense_3/bias\n\t [[Node: dense_3/bias/read = Identity[T=DT_FLOAT, _class=[\"loc:@dense_3/bias\"], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](dense_3/bias)]]\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "\n",
    "validate_every_n_steps = 10\n",
    "\n",
    "# MonitoredTrainingSession example\n",
    "with tf.train.MonitoredTrainingSession(checkpoint_dir='./saved_training',\n",
    "                                       save_checkpoint_secs=10,\n",
    "                                       save_summaries_steps=2) as sess:\n",
    "    while not sess.should_stop():\n",
    "        _, step = sess.run([train_op, global_step])\n",
    "        # Check if it is time to compute validation loss\n",
    "        if step % validate_every_n_steps == 0:\n",
    "            # Compute validation loss\n",
    "            run_validation(val_loss, sess)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
