{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow input pipeline: large datasets and data augmentation\n",
    "\n",
    "## Introduction\n",
    "\n",
    "The new high level Dataset API makes it quite easy to deal with large datasets. No need to bother with queues anymore.\n",
    "\n",
    "This notebook gives a short example of how to use the Dataset API for:\n",
    "\n",
    "1. loading multiple files for each input example,\n",
    "2. data augmentation.\n",
    "\n",
    "Loading multiple files for each input example can be needed for several applications:\n",
    "\n",
    "* dealing with image sequences,\n",
    "* object detection with multiple cameras,\n",
    "* etc.\n",
    "\n",
    "Data augmentation is a classical problem in deep learning. However, the Dataset API documentation does not give any hint about how to achieve data augmentation. I am not sure that the method used below is the best for this task. If you find a better way, don't hesitate to tell me!\n",
    "\n",
    "__Note on tensorflow version:__\n",
    "\n",
    "> This notebook works with tensorflow 1.4. It uses the Dataset API from *tf.data*. In previous versions, the Dataset API was in *tf.contrib.data*. Although I did not test it, it should be possible to use this notebook with tensorflow 1.3 by replacing *tf.data* by *tf.contrib.data*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's go!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create some dummy data\n",
    "\n",
    "We are going to create some dummy data files. In a real application, use your own data files. Here, we will create 4 files: two groups of two files. Each file will contain a 3x3 matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "height = 3\n",
    "width = 3\n",
    "\n",
    "# Create random files containing raw matrix of shape 3x3\n",
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        # Create matrix with fake data\n",
    "        matrix = np.zeros((height,width)) + i + (j*10)\n",
    "        # Save matrix as raw float32 file\n",
    "        matrix.astype('float32').tofile('data/file_' + str(i) + '_' + str(j) + '.raw')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at one of our dummy data files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 11.  11.  11.]\n",
      " [ 11.  11.  11.]\n",
      " [ 11.  11.  11.]]\n"
     ]
    }
   ],
   "source": [
    "# Print one of the generated files for checking\n",
    "matrix = np.fromfile('data/file_1_1.raw', dtype=np.float32)\n",
    "matrix = matrix.reshape((height,width))\n",
    "print(matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, our dummy data file looks good."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create parser\n",
    "\n",
    "We will need a parser to read our examples from the data files.\n",
    "\n",
    "Here, each example will be created by stacking data coming from two different files and a label. Therefore, the parser arguments will be the two filenames and the label. It will return two tensors containing the example and the label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create parser\n",
    "# Args: filenames\n",
    "# Returns: tensor containing read and decoded element\n",
    "def _parse_data(filename0, filename1, label):\n",
    "    # Read and decode first file\n",
    "    matrix0 = tf.read_file(filename0)\n",
    "    matrix0 = tf.decode_raw(matrix0, out_type=tf.float32)\n",
    "    matrix0 = tf.reshape(matrix0, [height,width])\n",
    "    \n",
    "    # Read and decode second file\n",
    "    matrix1 = tf.read_file(filename1)\n",
    "    matrix1 = tf.decode_raw(matrix1, out_type=tf.float32)\n",
    "    matrix1 = tf.reshape(matrix1, [height,width])\n",
    "    \n",
    "    # Stack the two elements together\n",
    "    X = tf.stack([matrix0, matrix1])\n",
    "    \n",
    "    # Get label (you could implement more complex logic here if needed)\n",
    "    y = label\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create data augmentation function\n",
    "\n",
    "We will need a function to implement the data augmentation logic.\n",
    "\n",
    "This function takes one example (X,y), and returns a dataset with two examples: the original example, and a second one generated on the fly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Data augmentation function: create several examples from one example\n",
    "# Args: One example X,y\n",
    "# Returns: Dataset containing several examples, after data augmentation\n",
    "def _data_augment(X,y):\n",
    "    # Generate new data from example X\n",
    "    X0 = X\n",
    "    X1 = -X # Dummy data augmentation, but we could use any transformation we need. We could also generate more examples.\n",
    "    X_augmented = tf.stack([X0,X1])\n",
    "    \n",
    "    # Repeat y\n",
    "    y_augmented = tf.stack([y,y])\n",
    "    \n",
    "    dataset = tf.data.Dataset.from_tensor_slices((X_augmented, y_augmented))\n",
    "        \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### It's almost done\n",
    "\n",
    "Now that we have implemented our parsing and data augmentation logic, we can create the dataset.\n",
    "\n",
    "As you can see, with the TensorFlow Dataset API, it is really easy! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "####################################################################\n",
    "# Create dataset from files\n",
    "####################################################################\n",
    "\n",
    "# Get the filenames lists\n",
    "filenames0 = ['data/file_0_0.raw', 'data/file_0_1.raw']\n",
    "filenames1 = ['data/file_1_0.raw', 'data/file_1_1.raw']\n",
    "\n",
    "# Create tensorflow constant containing the filenames\n",
    "tf_filenames0 = tf.constant(filenames0)\n",
    "tf_filenames1 = tf.constant(filenames1)\n",
    "\n",
    "# Create labels dataset\n",
    "labels = tf.constant([15, 25])\n",
    "\n",
    "# Create dataset containing filenames and labels\n",
    "dataset = tf.data.Dataset.from_tensor_slices((tf_filenames0, tf_filenames1, labels))\n",
    "\n",
    "# Use our _parse_data function to read files and decode data\n",
    "dataset = dataset.map(_parse_data)\n",
    "\n",
    "#####################################################################\n",
    "# Data augmentation\n",
    "#####################################################################\n",
    "\n",
    "# Apply data augmentation\n",
    "dataset = dataset.interleave(_data_augment, cycle_length=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print the dataset\n",
    "\n",
    "We can now print the dataset to check that everything is working."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([[[ 0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.]],\n",
      "\n",
      "       [[ 1.,  1.,  1.],\n",
      "        [ 1.,  1.,  1.],\n",
      "        [ 1.,  1.,  1.]]], dtype=float32), 15)\n",
      "(array([[[-0., -0., -0.],\n",
      "        [-0., -0., -0.],\n",
      "        [-0., -0., -0.]],\n",
      "\n",
      "       [[-1., -1., -1.],\n",
      "        [-1., -1., -1.],\n",
      "        [-1., -1., -1.]]], dtype=float32), 15)\n",
      "(array([[[ 10.,  10.,  10.],\n",
      "        [ 10.,  10.,  10.],\n",
      "        [ 10.,  10.,  10.]],\n",
      "\n",
      "       [[ 11.,  11.,  11.],\n",
      "        [ 11.,  11.,  11.],\n",
      "        [ 11.,  11.,  11.]]], dtype=float32), 25)\n",
      "(array([[[-10., -10., -10.],\n",
      "        [-10., -10., -10.],\n",
      "        [-10., -10., -10.]],\n",
      "\n",
      "       [[-11., -11., -11.],\n",
      "        [-11., -11., -11.],\n",
      "        [-11., -11., -11.]]], dtype=float32), 25)\n",
      "End of dataset\n"
     ]
    }
   ],
   "source": [
    "#####################################################################\n",
    "# Print the dataset\n",
    "#####################################################################\n",
    "\n",
    "# create TensorFlow Iterator object\n",
    "iterator = dataset.make_one_shot_iterator()\n",
    "next_element = iterator.get_next()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "\n",
    "    # iterate over the dataset\n",
    "    while True:\n",
    "        try:\n",
    "            elem = sess.run(next_element)\n",
    "            print(elem)\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            print(\"End of dataset\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### It is working!\n",
    "\n",
    "We can see that parsing and data augmentation work well:\n",
    "\n",
    "* each example contains:\n",
    "  * two matrices coming from two different files\n",
    "  * a label\n",
    "* after each example, there is an augmented example (-X, y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
